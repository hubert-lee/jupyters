{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "00_cross_entropy.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/hubert-lee/jupyters/blob/master/00_cross_entropy.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "O-5gyO-Ogujr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import math\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kh11jiaBgxGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "b5f76a8c-507e-4e51-a7bf-3191c41bb544"
      },
      "cell_type": "code",
      "source": [
        "#\n",
        "# negative log likelihood vs corss entropy\n",
        "#\n",
        "\n",
        "# 입력은 데이터 3개, 레이블은 3개 : 3*3\n",
        "input = torch.randn(3, 3, requires_grad=True)\n",
        "# 1차원에 대해서 softmax한 후, log를 씌운다.\n",
        "log_soft_max=F.log_softmax(input, dim=1)\n",
        "# 정답 레이블\n",
        "target = torch.tensor([0, 2, 1])\n",
        "output = F.nll_loss(log_soft_max, target)\n",
        "print(\"nll loss : {}\".format(output))\n",
        "\n",
        "\n",
        "#\n",
        "# 동일한 입력에 대해서 cross entropy도 똑같이 나오는지 확인해보자.\n",
        "#\n",
        "output = F.cross_entropy(input, target)\n",
        "print(\"ce loss : {}\".format(output))\n",
        "\n",
        "\n",
        "#\n",
        "# negative log loss를 직접 구해보자\n",
        "#\n",
        "soft_max_result = F.softmax(input, dim=1)\n",
        "print(\"=== softmax result ===\")\n",
        "print(soft_max_result)\n",
        "\n",
        "# 정의는 정답레이블에 해당하는 로그값을 모두 더한뒤 전체 데이터 개수로 나누어주면 된다.\n",
        "manual_nll = 0\n",
        "for idx, true_value in enumerate(target):\n",
        "  val = soft_max_result[idx][true_value].item()\n",
        "  manual_nll += math.log(val)   # log == ln\n",
        " \n",
        "\n",
        "print(\"=== manual nll loss ===\")\n",
        "print(manual_nll/3 * -1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nll loss : 0.6664645075798035\n",
            "ce loss : 0.6664645075798035\n",
            "=== softmax result ===\n",
            "tensor([[0.2913, 0.6955, 0.0132],\n",
            "        [0.1340, 0.0846, 0.7814],\n",
            "        [0.1306, 0.5949, 0.2745]], grad_fn=<SoftmaxBackward>)\n",
            "=== manual nll loss ===\n",
            "0.666464445135858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pWXS5bpfiGB8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}